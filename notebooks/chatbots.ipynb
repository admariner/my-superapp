{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/DANIEL/daltunay\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.secrets import load_secrets\n",
    "\n",
    "load_secrets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.generative_ai.large_language_models import Chatbot, ChatbotRAG, ChatbotTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 17:50:22.299 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/DANIEL/Library/Caches/pypoetry/virtualenvs/daltunay-b5-VQK2U-py3.11/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    Use the following context and chat history to answer the question:\n",
      "    \n",
      "    <context>Talk to me like a child</context>\n",
      "    <chat_history>[]</chat_history>\n",
      "    <question>what is the goal of life?</question>\n",
      "    \n",
      "    Answer in French only\n",
      "    \u001b[0m\n",
      "Le but de la vie est une question profonde et complexe. Chacun peut avoir sa propre perspective sur ce sujet. Certains diront que le but de la vie est d'être heureux, d'autres peuvent dire que c'est de trouver le sens de sa propre existence. Il n'y a pas de réponse unique à cette question, mais ce qui est important, c'est de vivre avec amour, de prendre soin des autres et de profiter de chaque moment.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Le but de la vie est une question profonde et complexe. Chacun peut avoir sa propre perspective sur ce sujet. Certains diront que le but de la vie est d'être heureux, d'autres peuvent dire que c'est de trouver le sens de sa propre existence. Il n'y a pas de réponse unique à cette question, mais ce qui est important, c'est de vivre avec amour, de prendre soin des autres et de profiter de chaque moment.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot = Chatbot()\n",
    "chatbot.ask(query=\"what is the goal of life?\", context=\"Talk to me like a child\", language=\"French\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    Use the following context and chat history to answer the question:\n",
      "    \n",
      "    <context>Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
      "encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\n",
      "the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\n",
      "the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
      "13\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14\n",
      "\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate</context>\n",
      "    <chat_history></chat_history>\n",
      "    <question>what is attention?</question>\n",
      "    \n",
      "    Answer in French only\n",
      "    \u001b[0m\n",
      "L'attention est un mécanisme utilisé dans les modèles de transduction de séquence pour relier différentes positions d'une séquence afin de calculer une représentation de la séquence. Il est utilisé dans des tâches telles que la compréhension de texte, la résumé abstrait, l'implication textuelle et l'apprentissage de représentations de phrases indépendantes de la tâche. L'attention permet de mettre l'accent sur certaines parties importantes de la séquence lors du traitement de l'information.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"L'attention est un mécanisme utilisé dans les modèles de transduction de séquence pour relier différentes positions d'une séquence afin de calculer une représentation de la séquence. Il est utilisé dans des tâches telles que la compréhension de texte, la résumé abstrait, l'implication textuelle et l'apprentissage de représentations de phrases indépendantes de la tâche. L'attention permet de mettre l'accent sur certaines parties importantes de la séquence lors du traitement de l'information.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot = ChatbotRAG()\n",
    "chatbot.ask(query=\"what is attention?\", language=\"French\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "{\n",
      "    \"action\": \"google_search\",\n",
      "    \"action_input\": \"Événements novembre 2023\"\n",
      "}\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"google_search\",\n",
      "    \"action_input\": \"Événements novembre 2023\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mUn événement festif pour découvrir des alternatives plus éthiques, locales et solidaires, à mettre au pied du sapin… Les spectacles et concerts à Paris en ... Nov 22, 2022 ... Novembre 2023 · 1er novembre : Toussaint (jour férié) / Journée mondiale végane / Mois anti-tabac / Movember · 3 novembre : Journée mondiale de ... Evénements de l'UIT : Novembre 2023 ; Me, Je ; 1, 2 ; add to Calendar ITU-T CE/GT 13 Suisse [Genève] Future networks. ÉVÉNEMENTS À VENIR. 15 novembre - 1 décembre 2023. Seminar Series with Yoko NIIMI (Doshisha University) in Paris. These 3 seminars with Yoko NIIMI (Doshisha ... Merci d'avoir participé à MongoDB.local Paris ! Les inscriptions sont maintenant closes. Voir les prochains évènements. novembre 2023Évènements & séminaires. Save the Date: Venture Capital Day 2023. Are you ready to explore the future of venture capital? Join us at the ... ... événements spéciaux qui auront lieu dans les parcs thématiques ... Heures et calendrier d'événements. samedi, novembre 25, 2023. Affichage de la ... MITJET ENDURANCE 4H - 1er NOVEMBRE 2023 Le championnat Mitjet International sera de retour sur le... 01. nov. STAGE DE PILOTAGE GTDRIVE - 1 ... Lundi 27 novembre, 18h30. Genève - Conférence Zéro Déchet. Lundi 27 novembre 2023 – de 18h30 à 19h30 – Uni Mail (salle MR060). Conférence-Rencontre. François ... Active gallery item: 1 of 2. À l'affiche. Raids Téracristal. 24 - 26 novembre, 1 - 3 décembre 2023. Affrontez un puissant Clamiral de Hisui ! En savoir plus.\u001b[0m\n",
      "Thought:{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"En novembre 2023, il y a eu plusieurs événements tels que des spectacles, des concerts, des conférences et des séminaires à Paris et dans d'autres endroits. Il y a également eu des journées mondiales comme la Journée mondiale végane, le Mois anti-tabac et Movember. De plus, il y a eu des événements sportifs comme le championnat Mitjet International et des activités spéciales dans les parcs thématiques. Pour plus d'informations sur des événements spécifiques, vous pouvez consulter les calendriers d'événements en ligne.\"\n",
      "}\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"En novembre 2023, il y a eu plusieurs événements tels que des spectacles, des concerts, des conférences et des séminaires à Paris et dans d'autres endroits. Il y a également eu des journées mondiales comme la Journée mondiale végane, le Mois anti-tabac et Movember. De plus, il y a eu des événements sportifs comme le championnat Mitjet International et des activités spéciales dans les parcs thématiques. Pour plus d'informations sur des événements spécifiques, vous pouvez consulter les calendriers d'événements en ligne.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"En novembre 2023, il y a eu plusieurs événements tels que des spectacles, des concerts, des conférences et des séminaires à Paris et dans d'autres endroits. Il y a également eu des journées mondiales comme la Journée mondiale végane, le Mois anti-tabac et Movember. De plus, il y a eu des événements sportifs comme le championnat Mitjet International et des activités spéciales dans les parcs thématiques. Pour plus d'informations sur des événements spécifiques, vous pouvez consulter les calendriers d'événements en ligne.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot = ChatbotTools(tool_names=[\"google-search\"])\n",
    "chatbot.ask(query=\"what happened in november 2023?\", language=\"French\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat-o-sophy-udGYv1To-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
