{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.secrets import load_secrets\n",
    "\n",
    "load_secrets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.generative_ai.large_language_models import Chatbot, ChatbotRAG, ChatbotTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 17:56:38.180 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/DANIEL/Library/Caches/pypoetry/virtualenvs/daltunay-b5-VQK2U-py3.11/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    Use the following context and chat history to answer the question:\n",
      "    \n",
      "    <context>Talk to me like a child</context>\n",
      "    <chat_history>[]</chat_history>\n",
      "    <question>what is the goal of life?</question>\n",
      "    \n",
      "    Answer in French only\n",
      "    \u001b[0m\n",
      "La réponse à la question \"quelle est la finalité de la vie ?\" peut varier d'une personne à l'autre, mais pour beaucoup, le but de la vie est de trouver le bonheur, l'épanouissement personnel et de vivre une vie remplie de sens et de satisfactions. Il est important de tirer le meilleur parti de chaque journée, de chérir les moments de bonheur et de cultiver de bonnes relations avec les autres. Chacun a la possibilité de choisir ses propres objectifs dans la vie et de travailler dur pour les atteindre. La vie est une aventure et il est important de la vivre de manière positive et significative.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'La réponse à la question \"quelle est la finalité de la vie ?\" peut varier d\\'une personne à l\\'autre, mais pour beaucoup, le but de la vie est de trouver le bonheur, l\\'épanouissement personnel et de vivre une vie remplie de sens et de satisfactions. Il est important de tirer le meilleur parti de chaque journée, de chérir les moments de bonheur et de cultiver de bonnes relations avec les autres. Chacun a la possibilité de choisir ses propres objectifs dans la vie et de travailler dur pour les atteindre. La vie est une aventure et il est important de la vivre de manière positive et significative.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot = Chatbot()\n",
    "chatbot.ask(query=\"what is the goal of life?\", context=\"Talk to me like a child\", language=\"French\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    Use the following context and chat history to answer the question:\n",
      "    \n",
      "    <context>Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
      "encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\n",
      "the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\n",
      "the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
      "13\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14\n",
      "\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate</context>\n",
      "    <chat_history></chat_history>\n",
      "    <question>what is attention?</question>\n",
      "    \n",
      "    Answer in French only\n",
      "    \u001b[0m\n",
      "L'attention est un mécanisme qui permet de relier différentes positions d'une séquence afin de calculer une représentation de cette séquence. Il est utilisé dans les modèles de transduction de séquence pour mettre en relation l'encodeur et le décodeur, et est souvent implémenté à l'aide de réseaux de neurones récurrents ou convolutionnels. Cependant, le Transformer est un modèle qui utilise uniquement l'attention pour calculer les représentations d'entrée et de sortie, sans recourir aux réseaux de neurones récurrents ou convolutionnels alignés sur la séquence.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"L'attention est un mécanisme qui permet de relier différentes positions d'une séquence afin de calculer une représentation de cette séquence. Il est utilisé dans les modèles de transduction de séquence pour mettre en relation l'encodeur et le décodeur, et est souvent implémenté à l'aide de réseaux de neurones récurrents ou convolutionnels. Cependant, le Transformer est un modèle qui utilise uniquement l'attention pour calculer les représentations d'entrée et de sortie, sans recourir aux réseaux de neurones récurrents ou convolutionnels alignés sur la séquence.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot = ChatbotRAG()\n",
    "chatbot.ask(query=\"what is attention?\", context=\"THIS IS THE CONTEXT!!!\", language=\"French\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "{\n",
      "    \"action\": \"google_search\",\n",
      "    \"action_input\": \"événements novembre 2023\"\n",
      "}\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"google_search\",\n",
      "    \"action_input\": \"événements novembre 2023\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mUn événement festif pour découvrir des alternatives plus éthiques, locales et solidaires, à mettre au pied du sapin… Les spectacles et concerts à Paris en ... Nov 22, 2022 ... Novembre 2023 · 1er novembre : Toussaint (jour férié) / Journée mondiale végane / Mois anti-tabac / Movember · 3 novembre : Journée mondiale de ... Evénements de l'UIT : Novembre 2023 ; Me, Je ; 1, 2 ; add to Calendar ITU-T CE/GT 13 Suisse [Genève] Future networks. ÉVÉNEMENTS À VENIR. 15 novembre - 1 décembre 2023. Seminar Series with Yoko NIIMI (Doshisha University) in Paris. These 3 seminars with Yoko NIIMI (Doshisha ... Merci d'avoir participé à MongoDB.local Paris ! Les inscriptions sont maintenant closes. Voir les prochains évènements. novembre 2023Évènements & séminaires. Save the Date: Venture Capital Day 2023. Are you ready to explore the future of venture capital? Join us at the ... ... événements spéciaux qui auront lieu dans les parcs thématiques ... Heures et calendrier d'événements. samedi, novembre 25, 2023. Affichage de la ... Tous les évènements. Tous; Concerts; Spectacles; Sports. Gims. Reporté au 11/02/25. Jeu ... Le Concert Événement. Sam. 25 Novembre 2023 à 20:00. Concert. Complet. Active gallery item: 1 of 2. À l'affiche. Raids Téracristal. 24 - 26 novembre, 1 - 3 décembre 2023. Affrontez un puissant Clamiral de Hisui ! En savoir plus. Événements ; Shaka Ponk. samedi 25 novembre 2023 ; Glorious. dimanche 26 novembre 2023 ; Prime Ochestra Rock Sympho Show. mardi 28 novembre 2023 - Annulé ; Okean ...\u001b[0m\n",
      "Thought:{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"En novembre 2023, il y aura différents événements tels que des spectacles, des concerts et des séminaires à Paris. Il y aura également des journées spéciales comme la Toussaint, la Journée mondiale végane, la Journée mondiale de l'UIT et le Mois anti-tabac. Pour plus d'informations, vous pouvez consulter les calendriers d'événements et effectuer une recherche en ligne.\"\n",
      "}\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"En novembre 2023, il y aura différents événements tels que des spectacles, des concerts et des séminaires à Paris. Il y aura également des journées spéciales comme la Toussaint, la Journée mondiale végane, la Journée mondiale de l'UIT et le Mois anti-tabac. Pour plus d'informations, vous pouvez consulter les calendriers d'événements et effectuer une recherche en ligne.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"En novembre 2023, il y aura différents événements tels que des spectacles, des concerts et des séminaires à Paris. Il y aura également des journées spéciales comme la Toussaint, la Journée mondiale végane, la Journée mondiale de l'UIT et le Mois anti-tabac. Pour plus d'informations, vous pouvez consulter les calendriers d'événements et effectuer une recherche en ligne.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot = ChatbotTools(tool_names=[\"google-search\"])\n",
    "chatbot.ask(query=\"what happened in november 2023?\", language=\"French\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat-o-sophy-udGYv1To-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
